

### **1. Introduction to ML**

#### **1.1 What is Learning?**

ML = Improving with experience.
üìß Spam filter = Learns from past emails ‚Üí Predicts new ones.

---

#### **1.2 Types of Learning**

| Type            | What it Learns From            | Example                   |
| --------------- | ------------------------------ | ------------------------- |
| Supervised      | Labeled data                   | Spam detection            |
| Unsupervised    | Unlabeled data                 | Customer grouping         |
| Semi-Supervised | Few labels + lots of unlabeled | Medical scans             |
| RL              | Rewards & penalties            | Robot learning to walk ü¶ø |

---

### **2. Well-Defined Learning Problem**

For ML to work, define:

* **T (Task)** ‚Äì What to do
* **P (Performance)** ‚Äì How to judge
* **E (Experience)** ‚Äì From where to learn

üí°Example:
T: Diagnose disease, P: Accuracy %, E: Doctor-labeled X-rays.

---

### **3. Designing a Learning System**

#### **3.1 Steps**

1. **Get Data** ‚Äì Labeled inputs/outputs (e.g., faces).
2. **Pick Features** ‚Äì What's useful (pixels, shapes).
3. **Choose Model** ‚Äì Like NN, DT, SVM.
4. **Train & Evaluate** ‚Äì Minimize errors.

#### **3.2 Spam Filter Example**

* üì© Data: Spam/ham emails
* üîç Features: Keywords, sender
* ü§ñ Model: Na√Øve Bayes
* üéØ Goal: Learn spam patterns

---

### **4. ML History Highlights**

üß† 1950s ‚Äì Turing dreams
‚öôÔ∏è 1980s ‚Äì Backprop hits
‚ôüÔ∏è 1997 ‚Äì Deep Blue wins chess
üß† 2012 ‚Äì Deep Learning explodes
üöó 2020s ‚Äì GPTs & self-driving rise

---

### **5. ANN Basics**

* üß† Inspired by brain
* üèóÔ∏è Layers: Input ‚Üí Hidden ‚Üí Output
* üì∏ Example: Detecting faces, cats, etc.


---

### **5.1 Artificial Neural Networks (ANN)**

* **Inspired by:** Human brain neurons.
* **Structure:**

  * **Input Layer:** Feeds data (e.g., image pixels).
  * **Hidden Layers:** Each neuron detects patterns‚Äîlike edges ‚Üí corners ‚Üí full objects.
  * **Output Layer:** Gives final prediction (e.g., cat, dog).
* **How it Learns:** Adjusts weights using **backpropagation** and **gradient descent** to reduce prediction error.
* **Use Case:** Facebook tagging, OCR, voice assistants.

---

### **5.2 Clustering (Unsupervised Learning)**

* **Goal:** Group unlabeled data based on similarity.
* **Algorithms:**

  * **K-Means:** Assigns each point to one of K centroids, updates centroids iteratively.
  * **Hierarchical:** Builds a tree (dendrogram), either top-down (divisive) or bottom-up (agglomerative).
* **Challenges:** Choosing the right K, interpreting results.
* **Use Case:** Customer segmentation, anomaly detection, topic modeling.

---

### **5.3 Reinforcement Learning (RL)**

* **Core Idea:** Agent learns by interacting with the environment.
* **Process:**

  * Takes action ‚Üí Gets reward/penalty ‚Üí Learns a **policy**.
* **Key Terms:**

  * **Policy:** Strategy of choosing actions.
  * **Value Function:** Expected return from a state.
  * **Q-Learning:** Estimates the value of action-state pairs.
* **Use Case:** AlphaGo, self-driving cars, game AI, robotic control.

---

### **5.4 Decision Tree Learning**

* **Concept:** If-else rules arranged in a tree structure.
* **Node Types:**

  * **Root Node:** First decision (e.g., age > 30?)
  * **Internal Nodes:** Further conditions.
  * **Leaves:** Final output/class label.
* **How it Learns:** Splits based on impurity (Gini, Entropy).
* **Use Case:** Loan approval, risk prediction, medical decisions.

---

### **5.5 Bayesian Networks**

* **Type:** Directed Acyclic Graph (DAG).
* **Nodes:** Variables (e.g., Rain, Wet Grass).
* **Edges:** Causal relationships (Rain ‚Üí Wet Grass).
* **Power:** Handles uncertainty + probabilistic inference.
* **Example:** Given wet grass, what's the probability it rained?
* **Use Case:** Disease diagnosis, spam detection, weather prediction.

---

### **5.6 Support Vector Machines (SVM)**

* **Goal:** Find a hyperplane that best separates classes with maximum margin.
* **Core Concepts:**

  * **Support Vectors:** Data points closest to the margin.
  * **Kernel Trick:** Maps data to higher dimensions (e.g., radial basis, polynomial).
* **Strengths:** Works well for small/medium datasets, high-dimensional data.
* **Use Case:** Spam detection, image classification, text categorization.

---

### **5.7 Genetic Algorithms**

* **Inspired by:** Natural evolution.
* **Process:**

  1. **Initialization:** Start with random population.
  2. **Fitness Evaluation:** Score each solution.
  3. **Selection:** Choose top performers.
  4. **Crossover:** Combine them to form new ‚Äúoffspring.‚Äù
  5. **Mutation:** Randomly tweak some genes.
* **Iterate:** Until optimal solution found or max generations hit.
* **Use Case:** Path optimization, game AI, neural network tuning.

---

## **6.1 Common Challenges in Machine Learning**

| **Issue**                  | **Description**                                                                                               | **Solution**                                                                                                 |
| -------------------------- | ------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------ |
| **Overfitting**            | Model performs well on training data but fails on unseen data. It **memorizes** instead of learning patterns. | Use **Regularization** (L1/L2), **Dropout** (NN), **Early stopping**, **Cross-validation**, **Prune trees**. |
| **Underfitting**           | Model is too simple, fails to capture underlying trend.                                                       | Use a **more complex model**, increase **features**, **train longer**.                                       |
| **Bias-Variance Tradeoff** | Balance between underfitting (high bias) and overfitting (high variance).                                     | Use **Cross-validation** to tune complexity and find sweet spot.                                             |
| **Poor Data Quality**      | Noisy, missing, inconsistent, or irrelevant data.                                                             | **Preprocessing**: clean data, impute missing values, remove outliers, normalize/scale.                      |
| **Class Imbalance**        | One class dominates (e.g., 95% not spam, 5% spam).                                                            | Use **resampling** (SMOTE, undersampling), **penalized models**, or **metrics** like F1-score.               |
| **Lack of Explainability** | Complex models like deep nets act as black boxes.                                                             | Use **SHAP, LIME, Decision Trees** for interpretable decisions.                                              |
| **Data Leakage**           | Info from test data unintentionally used in training.                                                         | Strictly separate training/validation/test sets.                                                             |

---

## **6.2 Data Science vs Machine Learning**

| **Aspect** | **Data Science**                                                  | **Machine Learning**                         |
| ---------- | ----------------------------------------------------------------- | -------------------------------------------- |
| **Scope**  | Broader field: statistics, visualization, business understanding. | Narrow focus: models that learn and predict. |
| **Goal**   | **Extract insights**, visualize, drive decisions.                 | **Build systems** that improve with data.    |
| **Tools**  | SQL, Pandas, Excel, Tableau, matplotlib.                          | Scikit-learn, TensorFlow, PyTorch, XGBoost.  |
| **Output** | Dashboards, reports, trends.                                      | Predictions, classifiers, clusters.          |
| **Skills** | Storytelling + analysis + communication.                          | Algorithm tuning + evaluation + math-heavy.  |

---

## üß† **Exam Cheat Sheet / Viva Answers**

* **Supervised vs Unsupervised Learning:**

  * Supervised = Labeled data (e.g., spam detection).
  * Unsupervised = Unlabeled data (e.g., clustering users).
  * Think: **"Teacher gives answers vs. students self-discovering groups."**

* **What is Overfitting?**

  * Problem: Model memorizes instead of generalizing.
  * Fix: Use **cross-validation**, **regularization**, **simpler models**, or **more data**.

* **Explain Reinforcement Learning:**

  * Agent learns by **rewards/punishments**.
  * Core idea: **Trial-and-error learning** to maximize reward.
  * Example: Game AI like AlphaGo, RL-based robotics.

* **Bias-Variance Tradeoff:**

  * High bias = Too simple ‚Üí underfitting.
  * High variance = Too complex ‚Üí overfitting.
  * Aim: Right balance = low total error.

---
